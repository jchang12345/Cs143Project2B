Question 1:

Functional Dependencies in labeled_data.csv:

Input_id -> labeldem
Input_id ->labelgop
Input_id ->labeldjt

Question 2:

No the data is not fully normalized. We could decompose it further by moving all attributes relating to the author into another relation and using an author_id. Also, we could decompose it further by moving all attributes relating to the subreddit into another relation and only using the subreddit_id. 
I think the data collector stored the data like this to make it easier to perform statistics on the data (A OLAP point of view, this is better to do aggregation functions). It would be easier because we don't need to perform joins. Also, it may be that the data was made available in this format and so it was just collected as is. The data is also thus, going to take up a lot more space.


Question 3:

Using explain on this line of code in reddit_model.py:

labeled_comments = labels.join(comments, comments.id == labels.Input_id).select(['Input_id', 'labeldem', 'labelgop', 'labeldjt', 'body']).explain()

Gave the output:
== Physical Plan ==
*(2) Project [Input_id#170, labeldem#171, labelgop#172, labeldjt#173, body#4]
+- *(2) BroadcastHashJoin [Input_id#170], [id#14], Inner, BuildLeft
   :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]))
   :  +- *(1) Filter isnotnull(Input_id#170)
   :     +- *(1) Sample 0.0, 0.1, false, -4372746341945787401
   :        +- *(1) FileScan parquet [Input_id#170,labeldem#171,labelgop#172,labeldjt#173] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/media/sf_vm-shared/labels.parquet], PartitionFilters: [], PushedFilters: [], Rea dSchema: struct<Input_id:string,labeldem:int,labelgop:int,labeldjt:int>
   +- *(2) Filter isnotnull(id#14)
      +- *(2) Sample 0.0, 0.1, false, 7407911338482665907
         +- *(2) FileScan parquet [body#4,id#14] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/media/sf_vm-shared/comments.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<body:string,id:string>
-

The join algorithms used by spark seem to be a broadcast hash join. Hash joins are generally useful for equality joins on large tables because it computes the join condition based on a hash index. It seems like it is also joining on the hash keys denoted by the number next to the attribute (input_id is 170 and id is 14) and does an inner left join. It is also worth noting that we only used 10% of the data to sampling just to speed things up a bit. The project on the first line after the physical plan shows SQL select being used or in relational algebra terms, the project statement to select the following 5 columns.




USAGE FOR PROJECT:
 spark-submit reddit_model.py